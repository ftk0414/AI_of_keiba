{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|            Variable Name|    Memory|\n",
      " ------------------------------------ \n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "\n",
    "print(\"{}{: >25}{}{: >10}{}\".format('|','Variable Name','|','Memory','|'))\n",
    "print(\" ------------------------------------ \")\n",
    "for var_name in dir():\n",
    "    if not var_name.startswith(\"_\") and sys.getsizeof(eval(var_name)) > 1000000: #ここだけアレンジ\n",
    "        print(\"{}{: >25}{}{: >10}{}\".format('|',var_name,'|',sys.getsizeof(eval(var_name)),'|'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use(\"ggplot\")\n",
    "import pandas as pd\n",
    "import time\n",
    "import datetime\n",
    "from tqdm.notebook import tqdm\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from urllib.request import urlopen\n",
    "import xml.etree.ElementTree as et\n",
    "from lxml import etree\n",
    "from dateutil import parser\n",
    "from selenium.webdriver import Chrome, ChromeOptions\n",
    "import chromedriver_binary\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "\n",
    "\n",
    "def update_data(old,new):\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old,new])\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_rows', 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for ids in (2023060105,2023060106,2023070105,2023070106,2023100101,2023100102):\n",
    "    for r in range(1, 13, 1):\n",
    "        race_id = (str(ids) + str(r).zfill(2))\n",
    "        race_id_list.append(race_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "race_id_list = []\n",
    "for year in range(2012,2014,1):\n",
    "    for place in range(1, 11, 1):\n",
    "        for kai in range(1, 13, 1):\n",
    "            for day in range(1, 13, 1):\n",
    "                for r in range(1, 13, 1):\n",
    "                    race_id = (\n",
    "                        str(year)\n",
    "                        + str(place).zfill(2)\n",
    "                        + str(kai).zfill(2)\n",
    "                        + str(day).zfill(2)\n",
    "                        + str(r).zfill(2)\n",
    "                    )\n",
    "                    race_id_list.append(race_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "old = '0331'\n",
    "new = '0407'\n",
    "\n",
    "def scrape_kaisai_date(from_: str, to_: str):\n",
    "    \"\"\"\n",
    "    yyyy-mmの形式でfrom_とto_を指定すると、間のレース開催日一覧が返ってくる関数。\n",
    "    to_の月は含まないので注意。\n",
    "    \"\"\"\n",
    "    print('getting race date from {} to {}'.format(from_, to_))\n",
    "    # 間の年月一覧を作成\n",
    "    date_range = pd.date_range(start=from_, end=to_, freq=\"W\")\n",
    "    # 開催日一覧を入れるリスト\n",
    "    kaisai_date_list = []\n",
    "    for year, month in tqdm(zip(date_range.year, date_range.month), total=len(date_range)):\n",
    "        #取得したdate_rangeから、スクレイピング対象urlを作成する。\n",
    "        #urlは例えば、https://race.netkeiba.com/top/calendar.html?year=2022&month=7 のような構造になっている。\n",
    "        query = [\n",
    "            'year=' + str(year),\n",
    "            'month=' + str(month),\n",
    "        ]\n",
    "        url = 'https://race.netkeiba.com/top/calendar.html' + '?' + '&'.join(query)\n",
    "        html = urlopen(url).read()\n",
    "        time.sleep(0.1)\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        a_list = soup.find('table', class_='Calendar_Table').find_all('a')\n",
    "        for a in a_list:\n",
    "            kaisai_date_list.append(re.findall('(?<=kaisai_date=)\\d+', a['href'])[0])\n",
    "        \n",
    "        from_ = from_.replace('-', '')\n",
    "        to_ = to_.replace('-', '')\n",
    "        kaisai_date_list = [date for date in kaisai_date_list if from_ <= date <= to_]\n",
    "    return kaisai_date_list\n",
    "    \n",
    "def scrape_race_id_list(kaisai_date_list: list, from_shutuba=False, waiting_time=10):\n",
    "    \"\"\"\n",
    "    開催日をyyyymmddの文字列形式でリストで入れると、レースid一覧が返ってくる関数。\n",
    "    レース前日準備のためrace_idを取得する際には、from_shutuba=Trueにする。\n",
    "    ChromeDriverは要素を取得し終わらないうちに先に進んでしまうことがあるので、その場合の待機時間をwaiting_timeで指定。\n",
    "    \"\"\"\n",
    "    race_id_list = []\n",
    "    options = ChromeOptions()\n",
    "    driver = Chrome(options=options)\n",
    "    #画面サイズをなるべく小さくし、余計な画像などを読み込まないようにする\n",
    "    driver.set_window_size(8, 8)\n",
    "    print('getting race_id_list')\n",
    "    for kaisai_date in tqdm(kaisai_date_list):\n",
    "        try:\n",
    "            query = [\n",
    "                'kaisai_date=' + str(kaisai_date)\n",
    "            ]\n",
    "            url = 'https://race.netkeiba.com/top/race_list.html' + '?' + '&'.join(query)\n",
    "            print('scraping: {}'.format(url))\n",
    "            driver.get(url)\n",
    "            try:\n",
    "                # 取得し終わらないうちに先に進んでしまうのを防ぐ\n",
    "                time.sleep(1)\n",
    "                a_list = driver.find_element(By.CLASS_NAME, 'RaceList_Box').find_elements(By.TAG_NAME, 'a')\n",
    "            except:\n",
    "                #それでも取得できなかったらもう10秒待つ\n",
    "                print('waiting more {} seconds'.format(waiting_time))\n",
    "                time.sleep(waiting_time)\n",
    "                a_list = driver.find_element(By.CLASS_NAME, 'RaceList_Box').find_elements(By.TAG_NAME, 'a')\n",
    "            for a in a_list:\n",
    "                if from_shutuba:\n",
    "                    race_id = re.findall('(?<=shutuba.html\\?race_id=)\\d+', a.get_attribute('href'))\n",
    "                else:\n",
    "                    race_id = re.findall('(?<=result.html\\?race_id=)\\d+', a.get_attribute('href'))\n",
    "                if len(race_id) > 0:\n",
    "                    race_id_list.append(race_id[0])\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    driver.close()\n",
    "    return race_id_list\n",
    "\n",
    "def scrape_race_results(race_id_list, pre_race_results={}):\n",
    "    race_results = pre_race_results\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            df = pd.read_html(url)[0]\n",
    "\n",
    "            # horse_idとjockey_idをスクレイピング\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "            \n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\"a\", attrs={\"href\": re.compile(\"^/horse\")})\n",
    "            for a in horse_a_list:\n",
    "                horse_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                horse_id_list.append(horse_id[0])\n",
    "            \n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\"a\", attrs={\"href\": re.compile(\"^/jockey\")})\n",
    "            for a in jockey_a_list:\n",
    "                jockey_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                jockey_id_list.append(jockey_id[0])\n",
    "                \n",
    "            # trainer_id\n",
    "            trainer_id_list = []\n",
    "            trainer_a_list = soup.find(\"table\", attrs={\"summary\": \"レース結果\"}).find_all(\"a\", attrs={\"href\": re.compile(\"^/trainer\")})\n",
    "            for a in trainer_a_list:\n",
    "                trainer_id = re.findall(r\"\\d+\", a[\"href\"])\n",
    "                trainer_id_list.append(trainer_id[0])\n",
    "\n",
    "            df[\"horse_id\"] = horse_id_list\n",
    "            df[\"jockey_id\"] = jockey_id_list\n",
    "            df[\"trainer_id\"] = trainer_id_list\n",
    "            race_results[race_id] = df\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_results\n",
    "\n",
    "def scrape_race_info(race_id_list,pre_race_infos= {}):\n",
    "    race_infos = pre_race_infos\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in race_infos.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            html = requests.get(url)\n",
    "            html.encoding = \"EUC-JP\"\n",
    "            soup = BeautifulSoup(html.text, \"html.parser\")\n",
    "\n",
    "            texts = (\n",
    "                soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[0].text\n",
    "                +soup.find(\"div\", attrs={\"class\": \"data_intro\"}).find_all(\"p\")[1].text\n",
    "            )\n",
    "            info = re.findall(r'\\w+', texts) #Qiitaでバックスラッシュを打つとバグるので大文字にしてあります。\n",
    "            info_dict = {}\n",
    "            for text in info:\n",
    "                if text in [\"芝\", \"ダート\"]:\n",
    "                    info_dict[\"race_type\"] = text\n",
    "                if \"障\" in text:\n",
    "                    info_dict[\"race_type\"] = \"障害\"\n",
    "                if \"m\" in text:\n",
    "                    info_dict[\"course_len\"] = int(re.findall(r\"\\d+\", text)[-1]) #0 ここも同様に大文字にしてます。\n",
    "                if text in [\"良\", \"稍重\", \"重\", \"不良\"]:\n",
    "                    info_dict[\"ground_state\"] = text\n",
    "                if text in [\"曇\", \"晴\", \"雨\", \"小雨\", \"小雪\", \"雪\"]:info_dict[\"weather\"] = text\n",
    "                if \"年\" in text:\n",
    "                    info_dict[\"date\"] = text\n",
    "            race_infos[race_id] = info_dict\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            break\n",
    "    return race_infos\n",
    "\n",
    "def scrape_horse_birth(horse_id_list, pre={}):\n",
    "    horse_birth = pre\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in horse_birth.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            df = pd.read_html(url)[1]\n",
    "            horse_birth[horse_id] = df[1][0]\n",
    "            time.sleep(0.01)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_birth\n",
    "\n",
    "def scrape_horse_results(horse_id_list, pre={}):\n",
    "    horse_results = pre\n",
    "    session =  requests.Session()\n",
    "    login_data = {'login_id':'hajimefutaki7946@gmaik.com','pswd':'4Wr2m3Tj3DCMmr5'}\n",
    "    session.post('https://regist.netkeiba.com/account/?pid=login&action=auth',login_data)\n",
    "    for horse_id in tqdm(horse_id_list):\n",
    "        if horse_id in horse_results.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = 'https://db.netkeiba.com/horse/' + horse_id\n",
    "            html = session.get(url)\n",
    "            df = pd.read_html(html.content)[3]\n",
    "            #df = pd.read_html(url)[3]\n",
    "            if df.columns[0]=='受賞歴':\n",
    "                df = pd.read_html(html.content)[4]\n",
    "            horse_results[horse_id] = df\n",
    "            time.sleep(0.01)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "            print(e)\n",
    "            break\n",
    "        except:\n",
    "            break\n",
    "    return horse_results\n",
    "\n",
    "def scrape_return_tables(race_id_list, pre_return_tables={}):\n",
    "    return_tables = pre_return_tables\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in return_tables.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            f = urlopen(url)\n",
    "            html = f.read()\n",
    "            html = html.replace(b'<br />', b'br')\n",
    "            dfs = pd.read_html(html)\n",
    "            return_tables[race_id] = pd.concat([dfs[1], dfs[2]])\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except:\n",
    "            break\n",
    "    return return_tables\n",
    "\n",
    "def scrape_jockey_results(jockey_id_list, pre={}):\n",
    "    jockey_results = pre\n",
    "    for jockey_id in tqdm(jockey_id_list):\n",
    "        if jockey_id in jockey_results.keys():\n",
    "            continue\n",
    "        for no in range(1,3,1):\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/?pid=jockey_detail&id=' + jockey_id + '&page=' + str(no)\n",
    "                df = pd.read_html(url)[0]\n",
    "                jockey_results[jockey_id,no] = df\n",
    "                time.sleep(0.01)\n",
    "            except IndexError:\n",
    "                #print('e1')\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                #print('e2')\n",
    "                continue\n",
    "    return jockey_results\n",
    "\n",
    "def scrape_trainer_results(trainer_id_list, pre={}):\n",
    "    trainer_results = pre\n",
    "    for trainer_id in tqdm(trainer_id_list):\n",
    "        if trainer_id in trainer_results.keys():\n",
    "            continue\n",
    "        for no in range(1,3,1):\n",
    "            try:\n",
    "                url = 'https://db.netkeiba.com/?pid=trainer_detail&id=' + trainer_id + '&page=' + str(no)\n",
    "                df = pd.read_html(url)[0]\n",
    "                trainer_results[trainer_id,no] = df\n",
    "                time.sleep(0.01)\n",
    "            except IndexError:\n",
    "                #print('e1')\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                #print('e2')\n",
    "                continue\n",
    "    return trainer_results\n",
    "\n",
    "def scrape_race_card_table(race_id_list):\n",
    "    data = pd.DataFrame()\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + race_id\n",
    "        df = pd.read_html(url)[0]\n",
    "        df = df.T.reset_index(level=0,drop=True).T\n",
    "        \n",
    "        html = requests.get(url)\n",
    "        html.encoding = 'EUC-JP'\n",
    "        soup = BeautifulSoup(html.text,'html.parser')\n",
    "        \n",
    "        texts = soup.find('div',attrs={'class':'RaceData01'}).text\n",
    "        texts = re.findall(r'\\w+',texts)\n",
    "        df['発走時刻'] = [texts[0]+':'+texts[1][:2]] * len(df)\n",
    "        for text in texts:\n",
    "            if 'm' in text:\n",
    "                df['course_len'] = [str(re.findall(r'\\d+',text)[0])] * len(df)\n",
    "            #if text in ['曇','晴','雨','小雨','小雪','雪']:\n",
    "            #    df['weather'] = [text] * len(df)\n",
    "            #if text in ['良', '稍重', '重']:\n",
    "            #    df['ground_state'] = [text] * len(df)\n",
    "            #if '不' in text:\n",
    "            #    df['ground_state'] = ['不良'] * len(df)\n",
    "            #if '稍' in text:\n",
    "            #    df['ground_state'] = ['稍重'] * len(df)\n",
    "            if '芝' in text:\n",
    "                df['R_type'] = ['芝'] * len(df)\n",
    "            if '障' in text:\n",
    "                df['R_type'] = ['障害'] * len(df)\n",
    "            if 'ダ' in text:\n",
    "                df['R_type'] = ['ダート'] * len(df)\n",
    "                \n",
    "        race_num = soup.find('span',attrs={'class':'RaceNum'}).text\n",
    "        df['R'] = re.findall(r'\\w+',race_num) * len(df)\n",
    "        \n",
    "        race_name = soup.find('div',attrs={'class':'RaceName'}).text\n",
    "        df['race_name'] = re.findall(r'\\w+',race_name) * len(df)\n",
    "        \n",
    "        date = soup.find('dd',attrs={'class':'Active'}).text\n",
    "        df['date'] = re.findall(r'\\d+/\\d+',date)[0]\n",
    "        \n",
    "        race_infos2 = soup.find('div',attrs={'class':'RaceData02'}).text\n",
    "        df['place'] = re.findall(r'\\w+',race_infos2)[1]\n",
    "        df['race_genre'] = re.findall(r'\\w+',race_infos2)[3]\n",
    "        df['race_grade'] = re.findall(r'\\w+',race_infos2)[4]\n",
    "        #df['頭数'] = re.findall(r'\\w+',race_infos2)[8]\n",
    "        \n",
    "        horse_id_list = []\n",
    "        horse_td_list = soup.find_all('td',attrs={'class':'HorseInfo'})\n",
    "        for td in horse_td_list:\n",
    "            horse_id = re.findall(r'\\d+',td.find('a')['href'])[0]\n",
    "            horse_id_list.append(horse_id)\n",
    "        \n",
    "        jockey_id_list = []\n",
    "        jockey_td_list = soup.find_all('td',attrs={'class':'Jockey'})\n",
    "        for td in jockey_td_list:\n",
    "            jockey_id = re.findall(r'\\d+',td.find('a')['href'])[0]\n",
    "            jockey_id_list.append(jockey_id)\n",
    "\n",
    "        trainer_id_list = []\n",
    "        trainer_td_list = soup.find_all('td',attrs={'class':'Trainer'})\n",
    "        for td in trainer_td_list:\n",
    "            trainer_id = re.findall(r'\\d+',td.find('a')['href'])[0]\n",
    "            trainer_id_list.append(trainer_id)\n",
    "        \n",
    "        df[\"horse_id\"] = horse_id_list\n",
    "        df['jockey_id'] = jockey_id_list\n",
    "        df[\"trainer_id\"] = trainer_id_list\n",
    "        df['place_id'] = df['place'].map(place_dict)\n",
    "        df['course_len_id'] = df['course_len']\n",
    "        df['R_type_id'] = df['R_type'].map(R_type_dict)\n",
    "        df['course_id'] = df['place_id']+df['R_type_id']+df['course_len_id']\n",
    "        df = df[['date','R','race_name','race_genre','race_grade','発走時刻','馬番','枠','horse_id','馬名','性齢','斤量','jockey_id','騎手','course_id','place','course_len','R_type']]\n",
    "        \n",
    "        df.index = [race_id]*len(df)\n",
    "        df['date'] = df.index.str[:4] +'/'+ df['date']\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        data = data.append(df)\n",
    "        \n",
    "        time.sleep(0.1)\n",
    "        \n",
    "    return data\n",
    "\n",
    "place_dict = {'札幌':'01','函館':'02','福島':'03','新潟':'04','東京':'05','中山':'06','中京':'07','京都':'08','阪神':'09','小倉':'10'}\n",
    "R_type_dict = {'芝':'01','ダート':'00','ダ':'00','障':'02','障害':'02'}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape_race_id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting race date from 2023-03-31 to 2023-04-07\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200a8343361c44369306565d2456e1e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "getting race_id_list\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bfc52f7e00848cc9d7059770b8428a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20230401\n",
      "scraping: https://race.netkeiba.com/top/race_list.html?kaisai_date=20230402\n",
      "\n",
      "48\n"
     ]
    }
   ],
   "source": [
    "now = datetime.datetime.now()\n",
    "one_week_after = now + datetime.timedelta(weeks=1)\n",
    "one_week_before = now + datetime.timedelta(weeks=-1)\n",
    "\n",
    "to_ = str(now.date())\n",
    "#to_ = str(one_week_after.date())\n",
    "from_ = str(one_week_before.date())\n",
    "kaisai_date_list = sorted(list(set(scrape_kaisai_date(from_, to_))))\n",
    "with open('kaisai_date_list.txt', 'w') as f:\n",
    "    for item in kaisai_date_list:\n",
    "        f.write(str(item) + '\\n')\n",
    "\n",
    "race_id_list = list(set(scrape_race_id_list(kaisai_date_list)))\n",
    "print(len(race_id_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape_race_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c901bf40ea83493db79328f4dd0963bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f64ecccbf4314faf8da00b0b53c83f26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_results = scrape_race_results(race_id_list)\n",
    "\n",
    "for key in race_results:\n",
    "    race_results[key].index = [key] * len(race_results[key])\n",
    "\n",
    "race_results = pd.concat([race_results[key] for key in race_results], sort=False)\n",
    "race_infos = scrape_race_info(race_id_list)\n",
    "race_infos = pd.concat([pd.DataFrame(race_infos[key], index=[key]) for key in race_infos])\n",
    "\n",
    "for key in race_infos:\n",
    "    race_infos[key].index = [key] * len(race_infos[key])\n",
    "\n",
    "race_results = race_results.merge(race_infos, left_index=True, right_index=True, how='left')\n",
    "race_results.to_pickle('race_results_new.pickle')\n",
    "race_results_old = pd.read_pickle('race_results_2012-2023' + old + '.pickle')\n",
    "race_results_new = pd.read_pickle('race_results_new.pickle')\n",
    "race_results = update_data(race_results_old,race_results_new)\n",
    "race_results.to_pickle('race_results_2012-2023' + new + '.pickle')\n",
    "len(race_results.index.unique()) - len(race_id_list) - len(race_results_old.index.unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape horse_birth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be87e4c671dc4582b2083169070dcf63",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=639.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "horse_id_list = race_results_new['horse_id'].unique()\n",
    "len(horse_id_list)\n",
    "\n",
    "horse_birth = scrape_horse_birth(horse_id_list)\n",
    "\n",
    "horse_birth = pd.DataFrame(horse_birth,index=['birth']).T\n",
    "horse_birth['birth'] = pd.to_datetime(horse_birth['birth'], format=\"%Y年%m月%d日\")\n",
    "horse_birth.to_pickle('horse_birth_new.pickle')\n",
    "horse_birth_old = pd.read_pickle('horse_birth_2012-2023' + old + '.pickle')\n",
    "horse_birth_new = pd.read_pickle('horse_birth_new.pickle')\n",
    "horse_birth = update_data(horse_birth_old,horse_birth_new)\n",
    "horse_birth.to_pickle('horse_birth_2012-2023' + new + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape horse_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89c807529cb4811890635f40b1c27fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=639.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1271260"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "race_results_new = pd.read_pickle('race_results_new.pickle')\n",
    "horse_id_list = race_results_new['horse_id'].unique()\n",
    "len(horse_id_list)\n",
    "horse_results = scrape_horse_results(horse_id_list)\n",
    "\n",
    "for key in horse_results:\n",
    "    horse_results[key].index = [key] * len(horse_results[key])\n",
    "    \n",
    "df = pd.concat([horse_results[key] for key in horse_results])\n",
    "df.to_pickle('horse_results_new.pickle')\n",
    "horse_results_old = pd.read_pickle('horse_results_2012-2023' + old + '.pickle')\n",
    "horse_results_new = pd.read_pickle('horse_results_new.pickle')\n",
    "\n",
    "horse_results = update_data(horse_results_old,horse_results_new)\n",
    "horse_results.to_pickle('horse_results_2012-2023' + new + '.pickle')\n",
    "len(horse_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape return_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1feb1fc73d63472c8dd0a8df6a9e7a1c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "return_tables = scrape_return_tables(race_id_list)\n",
    "\n",
    "for key in return_tables:\n",
    "    return_tables[key].index = [key] * len(return_tables[key])\n",
    "    \n",
    "return_tables = pd.concat([return_tables[key] for key in return_tables])\n",
    "return_tables.to_pickle('return_tables_new.pickle')\n",
    "return_tables_old = pd.read_pickle('return_tables_2012-2023' + old + '.pickle')\n",
    "return_tables_new = pd.read_pickle('return_tables_new.pickle')\n",
    "\n",
    "return_tables = update_data(return_tables_old,return_tables_new)\n",
    "return_tables.to_pickle('return_tables_2012-2023' + new + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape jockey_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b757ac073fd48aaa559f93c499c74d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=438.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "race_results = pd.read_pickle('race_results_2012-2023' + new + '.pickle')\n",
    "jockey_id_list_all = race_results['jockey_id'].unique()\n",
    "len(jockey_id_list_all)\n",
    "jockey_results = scrape_jockey_results(jockey_id_list_all)\n",
    "jrbu = jockey_results.copy()\n",
    "for key in jockey_results:\n",
    "    jockey_results[key].index = [key] * len(jockey_results[key])\n",
    "\n",
    "jockey_results = pd.concat([jockey_results[key] for key in jockey_results], sort=False)\n",
    "jockey_results['id'] = jockey_results.index\n",
    "jockey_results['id'] = jockey_results[['id']].astype(str)\n",
    "jockey_results['id'] = jockey_results['id'].map(lambda x: str(x)[2:7])\n",
    "jockey_results.index.name = 'jockey_id'\n",
    "jockey_results = jockey_results.set_index('id')\n",
    "jockey_results.index.name = 'jockey_id'\n",
    "jockey_results.to_pickle('jockey_results_new.pickle')\n",
    "jockey_results_old = pd.read_pickle('jockey_results_2012-2023' + old + '.pickle')\n",
    "jockey_results_new = pd.read_pickle('jockey_results_new.pickle')\n",
    "\n",
    "jockey_results = pd.concat([jockey_results_old,jockey_results_new])\n",
    "jockey_results = jockey_results.drop_duplicates()\n",
    "jockey_results = jockey_results.sort_values(['jockey_id','日付'], ascending=[True, False])\n",
    "jockey_results.to_pickle('jockey_results_2012-2023' + new + '.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape trainer_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc12f74a51ab400b8f9b029b8f77ce33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "race_results = pd.read_pickle('race_results_2012-2023' + new + '.pickle')\n",
    "trainer_id_list_all = race_results['trainer_id'].unique()\n",
    "len(trainer_id_list_all)\n",
    "trainer_results = scrape_trainer_results(trainer_id_list_all)\n",
    "\n",
    "for key in trainer_results:\n",
    "    trainer_results[key].index = [key] * len(trainer_results[key])\n",
    "\n",
    "trainer_results = pd.concat([trainer_results[key] for key in trainer_results], sort=False)\n",
    "trainer_results['id'] = trainer_results.index\n",
    "trainer_results['id'] = trainer_results[['id']].astype(str)\n",
    "trainer_results['id'] = trainer_results['id'].map(lambda x: str(x)[2:7])\n",
    "trainer_results.index.name = 'trainer_id'\n",
    "trainer_results = trainer_results.set_index('id')\n",
    "trainer_results.index.name = 'trainer_id'\n",
    "trainer_results.to_pickle('trainer_results_new.pickle')\n",
    "trainer_results_old = pd.read_pickle('trainer_results_2012-2023' + old + '.pickle')\n",
    "trainer_results_new = pd.read_pickle('trainer_results_new.pickle')\n",
    "\n",
    "trainer_results = pd.concat([trainer_results_old,trainer_results_new])\n",
    "trainer_results = trainer_results.drop_duplicates()\n",
    "trainer_results = trainer_results.sort_values(['trainer_id','日付'], ascending=[True, False])\n",
    "trainer_results.to_pickle('trainer_results_2012-2023' + new + '.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape corner_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_corner_rank(race_id_list, pre_corner_ranks={}):\n",
    "    corner_ranks = pre_corner_ranks\n",
    "    for race_id in tqdm(race_id_list):\n",
    "        if race_id in corner_ranks.keys():\n",
    "            continue\n",
    "        try:\n",
    "            url = \"https://db.netkeiba.com/race/\" + race_id\n",
    "            f = urlopen(url)\n",
    "            html = f.read()\n",
    "            html = html.replace(b'<br />', b'br')\n",
    "            dfs = pd.read_html(html)\n",
    "            corner_ranks[race_id] = dfs[4]\n",
    "            time.sleep(0.1)\n",
    "        except IndexError:\n",
    "            continue\n",
    "        except:\n",
    "            break\n",
    "    return corner_ranks\n",
    "\n",
    "def convert_comma(s: str) -> str:\n",
    "    result = \"\"\n",
    "    bracket_level = 0\n",
    "    for c in s:\n",
    "        if c == \"(\":\n",
    "            result += c\n",
    "            bracket_level += 1\n",
    "        elif c == \")\":\n",
    "            result += c\n",
    "            bracket_level -= 1\n",
    "        elif c == \",\" and bracket_level > 0:\n",
    "            result += \":\"\n",
    "        else:\n",
    "            result += c\n",
    "    return result\n",
    "\n",
    "def parse_string(s: str) -> pd.DataFrame:\n",
    "    # 結果を保存するリスト\n",
    "    df_list = []\n",
    "    x = 0\n",
    "    y = 0\n",
    "    \n",
    "    # 文字列を 1 文字ずつ取り出す\n",
    "    i = 0\n",
    "    while i < len(s):\n",
    "        c = s[i]\n",
    "        if c == \"*\":\n",
    "            y += 1\n",
    "        elif c == \"(\":\n",
    "            x = 0\n",
    "            y -= 1\n",
    "        elif c == \")\":\n",
    "            x = 0\n",
    "            y -= 1\n",
    "        elif c == \":\":\n",
    "            x += 1\n",
    "        elif c == \",\":\n",
    "            y -= 1.5\n",
    "            x = 0\n",
    "        elif c == \"-\":\n",
    "            y -= 3\n",
    "        elif c == \"=\":\n",
    "            y -= 5\n",
    "        elif c.isdigit():\n",
    "            # 数字の場合は、数字を連続して取り出す\n",
    "            value = 0\n",
    "            while i < len(s) and s[i].isdigit():\n",
    "                value = value * 10 + int(s[i])\n",
    "                i += 1\n",
    "            # 結果を保存するリストに追加する\n",
    "            df_list.append({\"number\": value,\"x\": x, \"y\": y})\n",
    "            continue\n",
    "        i += 1\n",
    "            \n",
    "    # リストを DataFrame に変換して返す\n",
    "    return pd.DataFrame(df_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ce15cd194ea476c9b4641b6f3dd0ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=48.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "corner_ranks = scrape_corner_rank(race_id_list)\n",
    "\n",
    "for key in corner_ranks:\n",
    "    corner_ranks[key].index = [key] * len(corner_ranks[key])\n",
    "    \n",
    "corner_ranks = pd.concat([corner_ranks[key] for key in corner_ranks])\n",
    "corner_ranks.columns = ['corner', 'rank']\n",
    "corner_ranks['rank'] = corner_ranks['rank'].astype(str).transform(convert_comma)\n",
    "#corner_ranks = corner_ranks[corner_ranks[0]=='4コーナー'].drop(0,axis=1)\n",
    "corner_ranks.to_pickle('corner_ranks_new.pickle')\n",
    "df = pd.read_pickle('corner_ranks_2012-2023'+ old +'.pickle')\n",
    "df = pd.concat([df, corner_ranks],axis=0)\n",
    "df.to_pickle('corner_ranks_2012-2023'+ new +'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['corner']=='4コーナー']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c09ae790d4f94e2ca375eb666dbb58e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=38637.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "df_parse = pd.DataFrame()\n",
    "for i in tqdm(range(df.index.nunique())):\n",
    "    test = []\n",
    "    test = parse_string(df['rank'][i])\n",
    "    test.index = [df.index[i]] * len(test)\n",
    "    df_parse = pd.concat([df_parse,test])\n",
    "\n",
    "df_parse['race_id'] = df_parse.index\n",
    "df_parse.to_pickle('corner_ranks_parse_2012-2023'+ new +'.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_master = pd.read_pickle('horse_results_2012-2023'+new+'.pickle')\n",
    "hr = hr_master.copy()\n",
    "\n",
    "def extract_number(string):\n",
    "    # Use the search function to find the first occurrence of a sequence of one or more digits\n",
    "    result = re.search(r'\\d+$', string)\n",
    "    # If a match was found, extract the matched string and return it as an integer\n",
    "    if result:\n",
    "        return int(result.group())\n",
    "    # If no match was found, return None\n",
    "    return None\n",
    "#地方競争はここで除外\n",
    "hr['place'] = hr['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
    "hr['events'] = hr['開催'].str.extract(r'(^\\d+)')\n",
    "hr.dropna(subset=['R'], inplace=True)\n",
    "hr['R'] = hr['R'].astype(int)\n",
    "hr['R'] = hr['R'].apply(lambda x: str(x).zfill(2))\n",
    "hr['weeks'] = hr['開催'].apply(extract_number)\n",
    "hr.dropna(subset=['weeks'], inplace=True)\n",
    "hr['weeks'] = hr['weeks'].astype(int)\n",
    "hr['weeks'] = hr['weeks'].apply(lambda x: str(x).zfill(2))\n",
    "hr['race_id'] = hr['日付'].str[:4] + hr['place'] +hr['events'].str.zfill(2) + hr['weeks'] + hr['R']\n",
    "hr.drop(['place','events','weeks'], axis=1, inplace=True)\n",
    "hr['number'] = hr['馬番']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_test = hr.merge(df_parse, on=['race_id','number'],how=\"left\")\n",
    "hr_test.index = hr.index\n",
    "hr_test.drop(['number'], axis=1, inplace=True)\n",
    "hr_test.to_pickle('horse_results_corner_rankd_merge_2012-2023'+new+'.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# scrape race_card_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc40fa23662a4598b27535eeddd2d606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=72.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-2209159658a0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mscrape_race_card_table\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrace_id_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'race_card_table.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'race_card_tables.pickle'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-55b099ad6126>\u001b[0m in \u001b[0;36mscrape_race_card_table\u001b[0;34m(race_id_list)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m         \u001b[0mdate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'dd'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'Active'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mre\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfindall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mr'\\d+/\\d+'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0mrace_infos2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoup\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'div'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'class'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m'RaceData02'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "scrape_race_card_table(race_id_list).to_pickle('race_card_table.pickle')\n",
    "df = pd.read_pickle('race_card_tables.pickle')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del df, horse_birth,horse_birth_old,\\\n",
    "    jockey_results,jockey_results_new,jockey_results_old,\\\n",
    "    race_results,race_results_new,race_results_old,\\\n",
    "    return_tables,return_tables_old,\\\n",
    "    trainer_results,trainer_results_new,trainer_results_old,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>日付</th>\n",
       "      <th>開催</th>\n",
       "      <th>天気</th>\n",
       "      <th>R</th>\n",
       "      <th>レース名</th>\n",
       "      <th>映像</th>\n",
       "      <th>頭数</th>\n",
       "      <th>枠番</th>\n",
       "      <th>馬番</th>\n",
       "      <th>オッズ</th>\n",
       "      <th>人気</th>\n",
       "      <th>着順</th>\n",
       "      <th>騎手</th>\n",
       "      <th>斤量</th>\n",
       "      <th>距離</th>\n",
       "      <th>馬場</th>\n",
       "      <th>馬場指数</th>\n",
       "      <th>タイム</th>\n",
       "      <th>着差</th>\n",
       "      <th>ﾀｲﾑ指数</th>\n",
       "      <th>通過</th>\n",
       "      <th>ペース</th>\n",
       "      <th>上り</th>\n",
       "      <th>馬体重</th>\n",
       "      <th>厩舎ｺﾒﾝﾄ</th>\n",
       "      <th>備考</th>\n",
       "      <th>勝ち馬(2着馬)</th>\n",
       "      <th>賞金</th>\n",
       "      <th>race_id</th>\n",
       "      <th>number</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2020103656</th>\n",
       "      <td>2022/12/11</td>\n",
       "      <td>6阪神4</td>\n",
       "      <td>晴</td>\n",
       "      <td>11</td>\n",
       "      <td>阪神ジュベナイルF(G1)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>18.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9</td>\n",
       "      <td>2.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>川田将雅</td>\n",
       "      <td>54.0</td>\n",
       "      <td>芝1600</td>\n",
       "      <td>良</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1:33.1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>100.0</td>\n",
       "      <td>8-8</td>\n",
       "      <td>33.7-36.1</td>\n",
       "      <td>35.5</td>\n",
       "      <td>462(-6)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>(シンリョクカ)</td>\n",
       "      <td>6633.0</td>\n",
       "      <td>202209060411</td>\n",
       "      <td>9</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020103656</th>\n",
       "      <td>2022/10/29</td>\n",
       "      <td>4東京8</td>\n",
       "      <td>晴</td>\n",
       "      <td>11</td>\n",
       "      <td>アルテミスS(G3)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>川田将雅</td>\n",
       "      <td>54.0</td>\n",
       "      <td>芝1600</td>\n",
       "      <td>良</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>1:33.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>88.0</td>\n",
       "      <td>6-6</td>\n",
       "      <td>35.8-33.8</td>\n",
       "      <td>33.3</td>\n",
       "      <td>468(+4)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ラヴェル</td>\n",
       "      <td>1210.2</td>\n",
       "      <td>202205040811</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020103656</th>\n",
       "      <td>2022/07/30</td>\n",
       "      <td>2新潟1</td>\n",
       "      <td>晴</td>\n",
       "      <td>05</td>\n",
       "      <td>2歳新馬</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "      <td>川田将雅</td>\n",
       "      <td>54.0</td>\n",
       "      <td>芝1600</td>\n",
       "      <td>良</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>1:35.8</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>75.0</td>\n",
       "      <td>7-7</td>\n",
       "      <td>38.2-32.0</td>\n",
       "      <td>31.4</td>\n",
       "      <td>464(0)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>出遅れ</td>\n",
       "      <td>(クルゼイロドスル)</td>\n",
       "      <td>700.0</td>\n",
       "      <td>202204020105</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-5.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    日付    開催 天気   R           レース名  映像    頭数   枠番  馬番  オッズ  \\\n",
       "2020103656  2022/12/11  6阪神4  晴  11  阪神ジュベナイルF(G1) NaN  18.0  5.0   9  2.6   \n",
       "2020103656  2022/10/29  4東京8  晴  11     アルテミスS(G3) NaN  10.0  3.0   3  1.4   \n",
       "2020103656  2022/07/30  2新潟1  晴  05           2歳新馬 NaN  12.0  2.0   2  2.1   \n",
       "\n",
       "             人気 着順    騎手    斤量     距離 馬場  馬場指数     タイム   着差  ﾀｲﾑ指数   通過  \\\n",
       "2020103656  1.0  1  川田将雅  54.0  芝1600  良 -13.0  1:33.1 -0.4  100.0  8-8   \n",
       "2020103656  1.0  2  川田将雅  54.0  芝1600  良 -21.0  1:33.9  0.1   88.0  6-6   \n",
       "2020103656  1.0  1  川田将雅  54.0  芝1600  良 -13.0  1:35.8 -0.5   75.0  7-7   \n",
       "\n",
       "                  ペース    上り      馬体重  厩舎ｺﾒﾝﾄ   備考    勝ち馬(2着馬)      賞金  \\\n",
       "2020103656  33.7-36.1  35.5  462(-6)     NaN  NaN    (シンリョクカ)  6633.0   \n",
       "2020103656  35.8-33.8  33.3  468(+4)     NaN  NaN        ラヴェル  1210.2   \n",
       "2020103656  38.2-32.0  31.4   464(0)     NaN  出遅れ  (クルゼイロドスル)   700.0   \n",
       "\n",
       "                 race_id  number    x    y  \n",
       "2020103656  202209060411       9  1.0 -4.0  \n",
       "2020103656  202205040811       3  1.0 -5.0  \n",
       "2020103656  202204020105       2  1.0 -5.5  "
      ]
     },
     "execution_count": 251,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hr_test.loc['2020103656']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hr_test)-len(hr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hr_master = pd.read_pickle('test_horse_results_2012-20221223.pickle')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>枠</th>\n",
       "      <th>馬番</th>\n",
       "      <th>印</th>\n",
       "      <th>馬名</th>\n",
       "      <th>性齢</th>\n",
       "      <th>斤量</th>\n",
       "      <th>騎手</th>\n",
       "      <th>厩舎</th>\n",
       "      <th>馬体重(増減)</th>\n",
       "      <th>Unnamed: 9_level_1</th>\n",
       "      <th>人気</th>\n",
       "      <th>登録</th>\n",
       "      <th>メモ</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ウイングレイテスト</td>\n",
       "      <td>牡6</td>\n",
       "      <td>57</td>\n",
       "      <td>松岡</td>\n",
       "      <td>美浦畠山</td>\n",
       "      <td>512(+2)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>エイシンチラー</td>\n",
       "      <td>牝5</td>\n",
       "      <td>55</td>\n",
       "      <td>Ｍデムーロ</td>\n",
       "      <td>美浦田中剛</td>\n",
       "      <td>482(+2)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>グラティアス</td>\n",
       "      <td>牡5</td>\n",
       "      <td>57</td>\n",
       "      <td>ムルザバエ</td>\n",
       "      <td>美浦宮田</td>\n",
       "      <td>506(+10)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>サクラトゥジュール</td>\n",
       "      <td>牡6</td>\n",
       "      <td>57</td>\n",
       "      <td>田辺</td>\n",
       "      <td>美浦堀</td>\n",
       "      <td>526(+2)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ワールドバローズ</td>\n",
       "      <td>牡5</td>\n",
       "      <td>57</td>\n",
       "      <td>石川</td>\n",
       "      <td>栗東石坂</td>\n",
       "      <td>460(0)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>インテンスライト</td>\n",
       "      <td>牡7</td>\n",
       "      <td>57</td>\n",
       "      <td>菊沢</td>\n",
       "      <td>美浦菊沢</td>\n",
       "      <td>502(+12)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ココロノトウダイ</td>\n",
       "      <td>牡6</td>\n",
       "      <td>58</td>\n",
       "      <td>バシュロ</td>\n",
       "      <td>美浦手塚</td>\n",
       "      <td>540(+2)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>シュヴァリエローズ</td>\n",
       "      <td>牡5</td>\n",
       "      <td>57</td>\n",
       "      <td>三浦</td>\n",
       "      <td>栗東清水久</td>\n",
       "      <td>454(-8)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ノルカソルカ</td>\n",
       "      <td>牡6</td>\n",
       "      <td>57</td>\n",
       "      <td>石橋脩</td>\n",
       "      <td>栗東藤岡</td>\n",
       "      <td>502(+4)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>7</td>\n",
       "      <td>10</td>\n",
       "      <td>NaN</td>\n",
       "      <td>キングエルメス</td>\n",
       "      <td>牡4</td>\n",
       "      <td>56</td>\n",
       "      <td>内田博</td>\n",
       "      <td>栗東矢作</td>\n",
       "      <td>496(0)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>アオイクレアトール</td>\n",
       "      <td>牡6</td>\n",
       "      <td>57</td>\n",
       "      <td>横山和</td>\n",
       "      <td>美浦古賀</td>\n",
       "      <td>500(+10)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>グランデマーレ</td>\n",
       "      <td>牡6</td>\n",
       "      <td>57</td>\n",
       "      <td>菅原明</td>\n",
       "      <td>栗東藤岡</td>\n",
       "      <td>506(-2)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ダノンチェイサー</td>\n",
       "      <td>牡7</td>\n",
       "      <td>58</td>\n",
       "      <td>津村</td>\n",
       "      <td>栗東池江</td>\n",
       "      <td>482(+4)</td>\n",
       "      <td>---.-</td>\n",
       "      <td>**</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1/7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    枠  馬番    印         馬名  性齢  斤量     騎手     厩舎   馬体重(増減) Unnamed: 9_level_1  \\\n",
       "0   1   1  NaN  ウイングレイテスト  牡6  57     松岡   美浦畠山   512(+2)              ---.-   \n",
       "1   2   2  NaN    エイシンチラー  牝5  55  Ｍデムーロ  美浦田中剛   482(+2)              ---.-   \n",
       "2   3   3  NaN     グラティアス  牡5  57  ムルザバエ   美浦宮田  506(+10)              ---.-   \n",
       "3   4   4  NaN  サクラトゥジュール  牡6  57     田辺    美浦堀   526(+2)              ---.-   \n",
       "4   4   5  NaN   ワールドバローズ  牡5  57     石川   栗東石坂    460(0)              ---.-   \n",
       "5   5   6  NaN   インテンスライト  牡7  57     菊沢   美浦菊沢  502(+12)              ---.-   \n",
       "6   5   7  NaN   ココロノトウダイ  牡6  58   バシュロ   美浦手塚   540(+2)              ---.-   \n",
       "7   6   8  NaN  シュヴァリエローズ  牡5  57     三浦  栗東清水久   454(-8)              ---.-   \n",
       "8   6   9  NaN     ノルカソルカ  牡6  57    石橋脩   栗東藤岡   502(+4)              ---.-   \n",
       "9   7  10  NaN    キングエルメス  牡4  56    内田博   栗東矢作    496(0)              ---.-   \n",
       "10  7  11  NaN  アオイクレアトール  牡6  57    横山和   美浦古賀  500(+10)              ---.-   \n",
       "11  8  12  NaN    グランデマーレ  牡6  57    菅原明   栗東藤岡   506(-2)              ---.-   \n",
       "12  8  13  NaN   ダノンチェイサー  牡7  58     津村   栗東池江   482(+4)              ---.-   \n",
       "\n",
       "    人気   登録   メモ date  \n",
       "0   **  NaN  NaN  1/7  \n",
       "1   **  NaN  NaN  1/7  \n",
       "2   **  NaN  NaN  1/7  \n",
       "3   **  NaN  NaN  1/7  \n",
       "4   **  NaN  NaN  1/7  \n",
       "5   **  NaN  NaN  1/7  \n",
       "6   **  NaN  NaN  1/7  \n",
       "7   **  NaN  NaN  1/7  \n",
       "8   **  NaN  NaN  1/7  \n",
       "9   **  NaN  NaN  1/7  \n",
       "10  **  NaN  NaN  1/7  \n",
       "11  **  NaN  NaN  1/7  \n",
       "12  **  NaN  NaN  1/7  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://race.netkeiba.com/race/shutuba.html?race_id=' + '202306010211'\n",
    "df = pd.read_html(url)[0]\n",
    "df = df.T.reset_index(level=0,drop=True).T\n",
    "\n",
    "html = requests.get(url)\n",
    "html.encoding = 'EUC-JP'\n",
    "soup = BeautifulSoup(html.text,'html.parser')\n",
    "date = soup.find('dd',attrs={'class':'Active'}).text\n",
    "df['date'] = re.findall(r'\\d+/\\d+',date)[0]\n",
    "df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "Can only use .str accessor with string values!",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-a8e4fffafcd4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m'/'\u001b[0m\u001b[0;34m+\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'date'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m#df['date'] = df['date'].str.replace('年','-').str.replace('月','-').str.replace('日','')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/accessor.py\u001b[0m in \u001b[0;36m__get__\u001b[0;34m(self, obj, cls)\u001b[0m\n\u001b[1;32m    185\u001b[0m             \u001b[0;31m# we're accessing the attribute of the class, i.e., Dataset.geo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    186\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 187\u001b[0;31m         \u001b[0maccessor_obj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_accessor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    188\u001b[0m         \u001b[0;31m# Replace the property with the accessor object. Inspired by:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         \u001b[0;31m# https://www.pydanny.com/cached-property.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   2098\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2099\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2100\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inferred_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2101\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_categorical\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mis_categorical_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2102\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_string\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"string\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/pandas/core/strings.py\u001b[0m in \u001b[0;36m_validate\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m   2155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minferred_dtype\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_types\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Can only use .str accessor with string values!\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2158\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minferred_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: Can only use .str accessor with string values!"
     ]
    }
   ],
   "source": [
    "df['date'] = df.index.str[:4] +'/'+ df['date']\n",
    "#df['date'] = df['date'].str.replace('年','-').str.replace('月','-').str.replace('日','')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "556be7b19251a39c80354168edc083d72d81e614836d97b4bf2b775d25f4698e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
